\label{sec: Estimation}

\subsection{Likelihood function}
In order to derive the proper likelihood of observing data generated by a cross-nested Data Generating Process (DGP), one need the derivative of \eqref{eq: G}. Fortunately we can easily derive this as
\begin {equation} \label{eq: Gi}
\begin{split}
\frac{\partial G}{\partial z_i} &= \sum_{m\in\mathcal{M}} \frac{\partial}{\partial z_i}
\left(
 \sum_{j\in\mathcal{C}} \alpha_{jm} z_j^{\mu_m}
 \right)^{\frac{\mu}{\mu_m}} \\
&= \sum_{m\in\mathcal{M}} \left[\frac{\mu}{\mu_m} \left(
\sum_{j\in\mathcal{C}} \alpha_{jm} z_j^{\mu_m}
\right)^{\frac{\mu}{\mu_m}-1}
 \cdot
\alpha_{im}\mu_m z_i^{\mu_m -1} \right]
\end{split}
\end{equation}
This gives us an analytical gradient of $G$ and by inserting it in the GEV definition $\textrm{Pr}(\mathcal{J} = i | \mathcal{C})$ (\ref{eq: probevolutiontwo}) \citep{bierlaire_theoretical_2006}. Rearranging terms we get
\begin{equation} \label{eq: likelihoodderiv}
\begin{split}
\textrm{Pr}(\mathcal{J} = i | \mathcal{C}) &=
\frac{z_i \left(
\mu \sum_{m} \alpha_{im} z_i^{\mu_m - 1}\left( \sum_j \alpha_{jm} z_j^{\mu_m} \right)^{\frac{\mu}{\mu_m} -1}
\right)}{ \mu
\sum_n \left(
\sum_j \alpha_{jn} z_j^{\mu_n}
\right)^{\frac{\mu}{\mu_n}}
}
\\
& = \frac{
 \sum_{m} \alpha_{im} z_i^{\mu_m }\left( \sum_j \alpha_{jm} z_j^{\mu_m} \right)^{\frac{\mu}{\mu_m} -1}
}{
\sum_n \left(
\sum_j \alpha_{jn} z_j^{\mu_n}
\right)^{\frac{\mu}{\mu_n}}
}
\end{split}
\end{equation}
where $n$ is a secondary nest index, as both $G(\cdot)$ and $G_i(\cdot)$ sums over all nests. Now adding one count of $\sum_j \alpha_{jm} z_j^{\mu_m}$ to the exponent in the numerator and using that the sums over $m$ and $n$ can be rearranged without concern as they are independent of each others indices, we arrive at
\begin{equation} \label{eq: likelihoodprob}
\textrm{Pr}(\mathcal{J} = i | \mathcal{C})  =
\sum_m
\frac{\left(
 \sum_j \alpha_{jm} z_j^{\mu_m} \right)^{\frac{\mu}{\mu_m}}}{\sum_n \left(
  \sum_j \alpha_{jn} z_j^{\mu_n} \right)^{\frac{\mu}{\mu_n}}
} \times
\frac{\alpha_{im}z_i^{\mu_m}}{\sum_j \alpha_{jm} z_j^{\mu_m}}
\end{equation}
This expression of the probability has a convenient interpretation as the summed probabilities of being in nest $m$ conditional on the choice set, times the probability of choosing option $i$ given that one is in nest $m$, that is
\begin{equation} \label{eq: CNL_conditional_probability}
\textrm{Pr}(\mathcal{J} = i | \mathcal{C})  = \sum_m \mathrm{Pr}(m|\mathcal{C}) \mathrm{Pr}(i|m)
\end{equation}
Which simply states that the probability of making any choice $i$ is the probability of choosing $i$ from nest $m$, summed over all nests with access to option $i$ \citep{bierlaire_theoretical_2006}.

Compared to the probability function for the NL \eqref{eq: NL_probevolutiontwo} the CNL \eqref{eq: likelihoodprob} is different as the sum over $j$ is over the whole choice set $\mathcal{C}$ and not only the within nest sets $\{\mathcal{C}_m\}_{m=1}^M$.  As mentioned this is countered simply by setting $\alpha_{im} = 0$ whenever choice $i$ is not in nest $m$.
\\ \\
Using the expression of probability given in \eqref{eq: likelihoodprob} it is relatively simple to derive the likelihood. Letting \eqref{eq: likelihoodprob} be the probability of observing an observation $c_i$, consequentially the product of these probabilities over the sample \textit{of observed outcomes} will represent the probability of observing the entire dataset within a CNL model parametrized with some parameters $\beta, \alpha, \mu_m, \mu$. Letting $d_{kj}$ be a dummy with value 1 if an individual $k$ chooses choice $j$ and $0$ otherwise, we can write the likelihood as
\begin{equation}
\mathcal{L}(\beta, \alpha, \mu_m, \mu | z) = \prod_{k=1}^K \textrm{Pr}(\mathcal{J} = i | \mathcal{C})^{d_{ki}}
\end{equation}
and as a direct extention thereof
\begin{equation}
\begin{split}
\ln \mathcal{L} (\beta, \alpha, \mu_m, \mu | z) &= \sum_{k=1}^K
d_{ki}
\ln \textrm{Pr}(\mathcal{J} = i | \mathcal{C})
\\
& = \sum_{k=1}^K d_{ki} \ln \left(
\sum_m
\frac{\left(
 \sum_j \alpha_{jm} z_j^{\mu_m} \right)^{\frac{\mu}{\mu_m}}}{\sum_n \left(
  \sum_j \alpha_{jn} z_j^{\mu_n} \right)^{\frac{\mu}{\mu_n}}
} \times
\frac{\alpha_{im}z_i^{\mu_m}}{\sum_j \alpha_{jm} z_j^{\mu_m}}
\right)
\end{split}
\end{equation}
Here also we see that the multinomial logit is fully contained in the CNL framework as when $\textrm{size}(\mathcal{C}) = 1$, $\mu_m = \mu = 1$ and $\forall j,m: \ \alpha_{jm} \in \{0,1\}$ the sums over nests can be dropped, resulting in the first fraction collapsing to a one. Due to the restrictions of parameters the second fraction will then exactly be the probability from the multinomial logit, and the entire expression is then the likelihood of a multinomial logit.
\\ \\
As noted by \citet{newman_computational_2018} the expression of the likelihood highlights that $\textrm{Pr}(\mathcal{J} = i| \mathcal{C})$ does not depend on individual $k$'s  actual choice, that is it does not depend on $d_{ki}$. This has computational benefits as it allows the calculation of the vector $\bm{P}(\mathcal{J} = i| \mathcal{C})$ of probabilities over all alternatives, and summing the subset of this vector where a dummy vector $\bm{d}_{k}$ is activated. By doing this the computation of $P$ can be vectorized, allowing dynamically typed code to approach the speeds achievable in more low-level code.

\subsection{Standard errors}
In this paper we only calculate standard errors when using pre-build software which is able to do so. In Biogeme standard errors are calculated as the Ramer-Crao lower bound $-E[\nabla^2 \mathcal{L}(\theta)]^{-1}$ \citep{bierlaire_pythonbiogeme:_2016}. Since this is dependent on the second derivatives of $\mathcal{L}$ it is infeasible to compute in python, without heavy optimization. There is no information available as to how Larch calculates standard errors, but considering that bootstrapping will be an extremely costly method, and that there are no easy ways to implement the delta method, we suspect Larch uses the same method as in Biogeme. As a sidenote the availability of relatively strong theoretical results on the CNL should make implementing standard errors using the Ramer-Crao lower bound relatively straight forward in faster languages.
\\ \\
After completing this initial step of calculating $\sigma_k = -E[\nabla^2 \mathcal{L}(\theta)]^{-1}$ it should then be straight forward to calculate t-statistics as $t_k = \beta_k/\sigma_k$ and p-values as $p = 2(1-\Phi(t_k))$. Standard errors on the marginal effects are probably easiest to calculate using the delta method.

\subsection{Marginal effects} \label{sec:marginalderivation}
To calculate marginal effects with respect to data $x$ the second derivative $\frac{\partial G_i}{\partial x}$ of \eqref{eq: G} is needed, as the first derivative is present in the expression of $\textrm{Pr}(i| \mathcal{C})$. This is found by (tedious) application of the chain rule to be
\begin{equation}
\begin{split}
  \frac{\partial^2 G}{\partial z_i \partial x} &= \sum_m \frac{\mu}{\mu_m}  \alpha_{im} \mu_m \bigg[
  \left( \frac{\mu}{\mu_m} - 1 \right) \left( \sum_j \alpha_{jm} z_j^{\mu_m} \right)^{\frac{\mu}{\mu_m} - 2} \cdot \left( \sum_j \alpha_{jm} z_j^{\mu_m - 1} \mu_m \beta_j \right) \cdot
 z_i^{\mu_m -1}
  \\
  & +
 \left( \sum_j \alpha_{jm} z_j^{\mu} \right)^{\frac{\mu}{\mu_m} - 1}  \cdot
  (\mu_m - 1) z_i^{\mu_m-2}\beta_i
  \bigg]
\end{split}
\end{equation}


% \begin{equation}
% \begin{split}
%   \frac{\partial^2 G}{\partial z_i \partial x} &= \sum_m \frac{\mu}{\mu_m} \alpha_{im} \mu_m z_i^{\mu_m - 1} \beta_i (\mu_m - 1) \bigg[ \left(\frac{\mu}{\mu_m} - 1\right) \left( \sum_j \alpha_{jm} z_j^{\mu_m} \right)^{\frac{\mu}{\mu_m}-1}  \\
%   &\times
%   \left(\sum_j \alpha_{jm}\mu_m z_j^{\mu_m -1} \beta_j(\mu_m -1) \right)
%   \bigg]
% \end{split}
% \end{equation}
With this result it is then possible to derive the marginal effects as
\begin{equation}
  \begin{split}
    \frac{\partial \textrm{Pr}(i|\mathcal{C})}{\partial x} &=
    \frac{
    \frac{\partial}{\partial x} \left[
        e^{x\beta_i}\frac{\partial G}{\partial z_i}
      \right]\cdot \left( \sum_j e^{x\beta_j} \frac{\partial G}{\partial z_j} \right) -
      e^{x\beta_i} \frac{\partial G}{\partial z_i} \frac{\partial}{\partial x}\left(
      \sum_j e^{x\beta_j} \frac{\partial G}{\partial z_j}
      \right)
    }{\left( \sum_j e^{x\beta_j} \frac{\partial G}{\partial z_j} \right)^2}
    \\
    &=
    \frac{
     e^{
     x\beta_i} \left(\beta_i \frac{\partial G}{\partial z_i} + \frac{\partial^2 G}{\partial z_i \partial x} \right)
     \cdot \left(\sum_j e^{x\beta_j} \frac{\partial G}{\partial z_j} \right)
     -
     e^{x\beta_i} \frac{\partial G}{\partial z_i} \sum_j  e^{x\beta_j} \left(\beta_j\frac{\partial G}{\partial z_j} + \frac{\partial^2 G}{\partial z_j \partial x} \right)
    }{\left( \sum_j e^{x\beta_j} \frac{\partial G}{\partial z_j} \right)^2}
    \\
    &= \beta_i \textrm{Pr}(i|\mathcal{C}) + \frac{e^{x\beta_i} \frac{\partial^2 G}{\partial z_i \partial x}}{\sum_j e^{x\beta_j} \frac{\partial G}{\partial z_j}}
    - \textrm{Pr}(i|\mathcal{C}) \cdot \sum_j \beta_j \textrm{Pr}(j|\mathcal{C})
    + \frac{e^{x\beta_j} \frac{\partial^2 G}{\partial z_j \partial x}}{\sum_{j'} e^{x\beta_{j'}} \frac{\partial G}{\partial z_{j'}}}
  \end{split}
\end{equation}
Recall that in the simply multinomial case these marginal effects can be derived to be $p_{i}\left(\beta_i - \sum_j p_j \beta_j \right)$, and that for the multinomial model, the first derivative of $G$ is 1, to see that this expression will collapse to the multinomial marginal effects under suitable restrictions. In general however we then have that
\begin{equation} \label{eq: marginaleffects}
    \frac{\partial \textrm{Pr}(i|\mathcal{C})}{\partial x} =
    \textrm{Pr}(i|\mathcal{C}) \left(
    \beta_i - \sum_j  \left[\beta_j \textrm{Pr}(j | \mathcal{C})  + \frac{e^{x\beta_j} \frac{\partial^2 G}{\partial z_j \partial x}}{\sum_{j'} e^{x\beta_{j'}} \frac{\partial G}{\partial z_{j'}}}  \right]    \right) +  \frac{e^{x\beta_i} \frac{\partial^2 G}{\partial z_i \partial x}}{\sum_j e^{x\beta_j} \frac{\partial G}{\partial z_j}}
\end{equation}

\subsection{Identification}
Due to the complexity of the generating function $G(z_1,...,z_J)$ (eq.\ref{eq: G}) the exact set of necessary constraints for model identification is not known. Though, in the literature there are proposed several different sufficient restrictions for model estimation that are either complementary or substitutes.
\\ \\
The sufficient conditions in section \ref{sec: GEV-conditions} for $G$ to be a GEV generating function should be applied. For an alternative $j$ belonging to $l$ nests it is common in application to fix the weights to $\alpha_{jm}=1/l$ (\cite{jong_discrete_2014}). Among others this approach is used by Stephane Hess et al (\citeyear{hess_joint_2012}) and motivated by the complexity of actually estimating the $\alpha$-coefficients.
\\ \\
Inserting the definition of $z_i=e^{V_i}$ into the probability function (\ref{eq: likelihoodprob}) and specifying $V_i = (\beta_i^0 + \psi) + x(\beta_i + \delta)$ gives a version of the likelihood where parameters are directly accesible, and where we can study some identification problems:


\begin{equation} \label{eq: psi}
  \footnotesize
  \begin{split}
    \textrm{Pr}(i | \mathcal{C})|_{V_i = (\beta_i^0 + \psi) + x(\beta_i + \delta)}  &=
    \sum_m
    \frac{
    \textcolor{blue}{\left(e^{\mu_m x\delta}\right)^\frac{\mu}{\mu_m}}
    \textcolor{blue}{\left(e^{\mu_m \psi}\right)^\frac{\mu}{\mu_m}}
    \left(\sum_j \alpha_{jm} e^{\mu_m(\beta^0_j+x\beta_j)} \right)^{\frac{\mu}{\mu_m}}}
    {\sum_n
    \textcolor{blue}{\left(e^{\mu_n x\delta}\right)^\frac{\mu}{\mu_n}}
    \textcolor{blue}{\left(e^{\mu_n \psi}\right)^\frac{\mu}{\mu_n}}
    \left(\sum_j \alpha_{jn} e^{\mu_n(\beta^0_j+x\beta_j)} \right)^{\frac{\mu}{\mu_n}}
    } \times
    \frac{
    \textcolor{blue}{e^{\mu_m x\delta}}
    \textcolor{blue}{e^{\mu_m \psi}}
    \alpha_{im}e^{\mu_m(\beta^0_i+x\beta_i)}}
    {\textcolor{blue}{e^{\mu_m x\delta}}
    \textcolor{blue}{e^{\mu_m \psi}}
     \sum_j \alpha_{jm} e^{\mu_m (\beta^0_j+x\beta_j)}}
    \\
    &=
    \textcolor{blue}{\frac{e^{\mu x\delta} }{e^{\mu x\delta}}}
    \textcolor{blue}{\frac{e^{\mu \psi} }{e^{\mu \psi}}}
    \sum_m
    \frac{\left(
     \sum_j \alpha_{jm} e^{\mu_m(\beta^0_j+x\beta_j)} \right)^{\frac{\mu}{\mu_m}}}{\sum_n \left(
      \sum_j \alpha_{jn} e^{\mu_n(\beta^0_j+x\beta_j)} \right)^{\frac{\mu}{\mu_n}}
    } \times
    \frac{\alpha_{im}e^{\mu_m(\beta^0_i+x\beta_i)}}{\sum_j \alpha_{jm} e^{\mu_m (\beta^0_j+x\beta_j)}}
    \\ &=   \textrm{Pr}(i | \mathcal{C})|_{V_i = \beta_i^0 + x\beta_i}
  \end{split}
  \end{equation}

Thus there is both a scale, and a location problem with identification. Regarding the MNL model there exists a common strategy for solving both by arbitrarily fixing $\beta^0_j=0$ for one of the alternatives $j$, and fixing one parameter of $\beta_i$ in each choice (\cite{ben-akiva_discrete_1985}).

For the NL model there exist two traditional approaches for solving the location-issue. The one known as the "elemental strategy" fixes $\beta^0_j=0$ for one of the elemental nodes (arbitrarily choosing one of the "dead ends") as well as for all of the structural nodes. The "structural strategy" on the other hand imposes $\beta^0_j=0$ for one arbitrary "child" of the root and each structural node of the tree. Bierlaire (\citeyear{bierlaire_overspecification_1997}) introduces the "ortogonal strategy" where the "constraint subspace" contains all the necessary constraints and is chosen in order to be orthogonal to the "invariant subspace" that contains all the overspecification due to ASCs. The main benefit of the orthogonal strategy is  that is reduces the chance that a maximization algorithm will stop too early and that it will arrive at different results depending on the arbitrary choices of $j$ for which $\beta^0_j=0$ (\cite{bierlaire_overspecification_1997}).
\\ \\
A separate issue is that of "utility-shifting" where adding and/or subtracting to utility respectively in nests and nest-children does not affect the overall utility of the nest because of the logsum term. In the NL models is a sufficient condition to fix $\beta^0_j$ and one of the parameters in the $\beta_j$ vector for each of the structural nodes. A simple proof of this can be found in appendix \ref{app:utilityshifting}, a more general approach is available in \citep{bierlaire_overspecification_1997}.
\\ \\
Knowledge of the exact restrictions required for identification of the CNL model is limited. With regards to the issue of utility shifting, the non-pure nesting structure makes the shifting many dimensional, as there is no longer one nest for each sub-choiceset $\mathcal{C}_m$. Thus one need to solve as many interdependent equations as there are cross-nested nests, to show identification. For the case of ACS, mathematically one needs to show that a parametrization satisfies
\begin{equation} \label{eq:utilityshifting}
  \begin{bmatrix}
     V_s^\psi \\
     \vdots \\
     V_d^\psi
  \end{bmatrix} =
\begin{bmatrix}
   H_s + \psi_s + \frac{1}{\mu_s} \ln \sum_j e^{V_j \mu_s - \psi_j} \\
   \vdots \\
  H_d + \psi_d + \frac{1}{\mu_d} \ln \sum_j e^{V_j \mu_d - \psi_j}
  \end{bmatrix} \neq
\begin{bmatrix}
   V_s \\
   \vdots \\
   V_d
\end{bmatrix}
\end{equation}
where $s...d$ are indices for all the nests. Unlike in NL where these can be solved separately, the cross nesting implies that the sum over $j$ will count the same $\psi_j$ in multiple equations, meaning some equations must be solved in pairs, triplets etc. It is this highly context-dependent nature of the problem that makes it hard.
